(function(t){function e(e){for(var i,s,n=e[0],l=e[1],c=e[2],p=0,d=[];p<n.length;p++)s=n[p],Object.prototype.hasOwnProperty.call(r,s)&&r[s]&&d.push(r[s][0]),r[s]=0;for(i in l)Object.prototype.hasOwnProperty.call(l,i)&&(t[i]=l[i]);h&&h(e);while(d.length)d.shift()();return o.push.apply(o,c||[]),a()}function a(){for(var t,e=0;e<o.length;e++){for(var a=o[e],i=!0,n=1;n<a.length;n++){var l=a[n];0!==r[l]&&(i=!1)}i&&(o.splice(e--,1),t=s(s.s=a[0]))}return t}var i={},r={app:0},o=[];function s(e){if(i[e])return i[e].exports;var a=i[e]={i:e,l:!1,exports:{}};return t[e].call(a.exports,a,a.exports,s),a.l=!0,a.exports}s.m=t,s.c=i,s.d=function(t,e,a){s.o(t,e)||Object.defineProperty(t,e,{enumerable:!0,get:a})},s.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})},s.t=function(t,e){if(1&e&&(t=s(t)),8&e)return t;if(4&e&&"object"===typeof t&&t&&t.__esModule)return t;var a=Object.create(null);if(s.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:t}),2&e&&"string"!=typeof t)for(var i in t)s.d(a,i,function(e){return t[e]}.bind(null,i));return a},s.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return s.d(e,"a",e),e},s.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},s.p="";var n=window["webpackJsonp"]=window["webpackJsonp"]||[],l=n.push.bind(n);n.push=e,n=n.slice();for(var c=0;c<n.length;c++)e(n[c]);var h=l;o.push([0,"chunk-vendors"]),a()})({0:function(t,e,a){t.exports=a("56d7")},"034f":function(t,e,a){"use strict";a("85ec")},"56d7":function(t,e,a){"use strict";a.r(e);a("e260"),a("e6cf"),a("cca6"),a("a79d");var i=a("2b0e"),r=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticStyle:{"background-color":"#EFEFEF"},attrs:{id:"app"}},[a("div",{attrs:{id:"abs"}},[a("link",{attrs:{rel:"stylesheet",href:"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"}}),t._m(0),a("h1",{staticStyle:{color:"#4484CE","margin-left":"70px","margin-top":"40px","margin-right":"70px","font-size":"2.5vw"}},[t._v(" Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects")]),t._m(1),t._m(2),a("h2",{attrs:{id:"Abstract"}},[t._v(" Abstract ")]),a("p",{staticStyle:{"text-align":"left","margin-left":"40px","margin-right":"40px"}},[t._v(" The basis of many object manipulation algorithms is RGB-D input. Yet,commodity RGB-D sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Toronto Transparent Object Depth Dataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental evaluation demonstrates that TranspareNet outperforms existing state-of-the-art depth completion methods on multiple datasets, including ClearGrasp, and that it also handles cluttered scenes when trained on TODD. ")]),a("div",{staticClass:"float-container"},[a("div",{staticClass:"float-child"},[a("v-btn",{staticClass:"ma-2",attrs:{href:"https://openreview.net/forum?id=tCfLLiP7vje"}},[t._v(" Paper")])],1),a("div",{staticClass:"float-child"},[a("v-btn",{staticClass:"ma-2",attrs:{href:"https://github.com/pairlab/TranspareNet"}},[t._v(" Code")])],1),a("div",{staticClass:"float-child"},[a("v-btn",{staticClass:"ma-2",attrs:{href:"https://doi.org/10.5683/SP3/ZJJAJ3"}},[t._v(" Dataset")])],1)]),t._m(3),a("div",{staticClass:"float-container",staticStyle:{"margin-left":"70px","margin-right":"70px"}},[a("div",{staticClass:"float-child-half"},[a("h2",{attrs:{id:"Architecture"}},[t._v(" TranspareNet ")]),t._v(" "),a("v-img",{attrs:{src:t.publicPath+"main.svg",width:"500",height:"300",contain:""}}),a("p",{attrs:{STYLE:"text-align:left;font-size:10.0pt;margin-right:40px;"}},[t._v(" Transparent object depth is de-projected to a point cloud, and put through Point Cloud Completion module to get the final point cloud. The final point cloud is then projected to the depth domain and replaces the original depth of the transparent object within the mask. An encoder-decoder based Depth Completion module takes combined depth and RGB signal as input, and the decoder modulation branch takes the object mask as input for modulation. Finally, the decoder outputs the predicted completed depth.")])],1),t._m(4)])]),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("br"),a("h2",{attrs:{id:"Dataset"}},[t._v(" Toronto Transparent Objects Depth Dataset (TODD) ")]),a("p",{staticStyle:{"text-align":"left","margin-left":"40px","margin-right":"40px"}},[t._v(" In total, Toronto Transparent Objects Depth Dataset (TODD) has 14,659 images of scenes which contains six glass beakers and flasks in five different backgrounds. Four objects are used in training set and the other two novel objects form the novel validation and test set. The training set has 10,302 images where validation and testing set combined has 4357 images. Every scene consists of up to three transparent objects with occlusion, which introduces additional complexity to the dataset. The objects and their placement are selected to mimic real-life transparent glassware, which can help to develop vision aware robots capable of manipulating transparent vessels.")]),a("div",{staticClass:"float-container",staticStyle:{"margin-left":"70px","margin-right":"70px"}},[a("div",{staticClass:"float-child"},[a("v-img",{attrs:{src:t.publicPath+"cad.png","max-width":"300","max-height":"200",contain:""}})],1),a("div",{staticClass:"float-child"},[a("v-img",{attrs:{src:t.publicPath+"glass.png","max-width":"300","max-height":"200",contain:""}})],1),a("div",{staticClass:"float-child"},[a("v-img",{attrs:{src:t.publicPath+"filled.png","max-width":"300","max-height":"200",contain:""}})],1)]),a("br",{staticStyle:{clear:"both"}}),a("v-banner",{attrs:{"single-line":""}},[a("h3",{attrs:{id:"Explorer"}},[t._v(" Dataset Explorer ")]),a("span",{attrs:{STYLE:"font-size:12.0pt"}},[t._v("Select Object Type: ")]),a("select",{directives:[{name:"model",rawName:"v-model",value:t.objType,expression:"objType"}],attrs:{STYLE:"font-size:12.0pt; background-color:#4484CE; border-radius: 10px;"},on:{change:function(e){var a=Array.prototype.filter.call(e.target.options,(function(t){return t.selected})).map((function(t){var e="_value"in t?t._value:t.value;return e}));t.objType=e.target.multiple?a:a[0]}}},[a("option",{attrs:{value:"0"}},[t._v("Beaker 0")]),a("option",{attrs:{value:"1"}},[t._v("Beaker 1")]),a("option",{attrs:{value:"2"}},[t._v("Beaker 2")]),a("option",{attrs:{value:"3"}},[t._v("Flask 0")]),a("option",{attrs:{value:"4"}},[t._v("Flask 1")]),a("option",{attrs:{value:"5"}},[t._v("Flask 2")])])]),a("div",{staticClass:"float-container"},[a("div",{staticClass:"float-child",attrs:{id:"imageGif"}},[a("h4",[t._v(" RGB ")]),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:0==t.objType,expression:"objType == 0"}],attrs:{width:"320",height:"200",file:"image0.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:1==t.objType,expression:"objType == 1"}],attrs:{width:"320",height:"200",file:"image1.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:2==t.objType,expression:"objType == 2"}],attrs:{width:"320",height:"200",file:"image2.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:3==t.objType||5==t.objType,expression:"objType == 3 || objType == 5"}],attrs:{width:"320",height:"200",file:"image53.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:4==t.objType,expression:"objType == 4"}],attrs:{width:"320",height:"200",file:"image24.gif"}})],1),a("div",{staticClass:"float-child",attrs:{id:"depthGif"}},[a("h4",[t._v(" Sensor Raw Depth ")]),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:0==t.objType,expression:"objType == 0"}],attrs:{width:"320",height:"200",file:"depth0.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:1==t.objType,expression:"objType == 1"}],attrs:{width:"320",height:"200",file:"depth1.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:2==t.objType,expression:"objType == 2"}],attrs:{width:"320",height:"200",file:"depth2.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:3==t.objType||5==t.objType,expression:"objType == 3 || objType == 5"}],attrs:{width:"320",height:"200",file:"depth53.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:4==t.objType,expression:"objType == 4"}],attrs:{width:"320",height:"200",file:"depth24.gif"}})],1),a("div",{staticClass:"float-child",attrs:{id:"depthGTGif"}},[a("h4",[t._v(" Ground Truth Depth ")]),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:0==t.objType,expression:"objType== 0"}],attrs:{width:"320",height:"200",file:"depthgt0.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:1==t.objType,expression:"objType == 1"}],attrs:{width:"320",height:"200",file:"depthgt1.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:2==t.objType,expression:"objType == 2"}],attrs:{width:"320",height:"200",file:"depthgt2.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:3==t.objType||5==t.objType,expression:"objType == 3 || objType == 5"}],attrs:{width:"320",height:"200",file:"depthgt53.gif"}}),a("gif-viewer",{directives:[{name:"show",rawName:"v-show",value:4==t.objType,expression:"objType == 4"}],attrs:{width:"320",height:"200",file:"depthgt24.gif"}})],1)]),a("div",{staticClass:"float-container"},[a("div",{staticClass:"float-child"},[a("h4",[t._v(" Object CAD Model ")]),a("model-stl",{attrs:{src:""+t.publicPath+t.objType+".stl",height:320,width:320,cameraPosition:t.scale}})],1),a("div",{staticClass:"float-child"},[a("h4",[t._v(" Raw Depth Point Cloud")]),a("model-ply",{attrs:{src:t.publicPath+"depth2pcd_"+t.objType+".ply",height:320,width:320}})],1),a("div",{staticClass:"float-child"},[a("h4",[t._v(" Ground Truth Depth Point Cloud")]),a("model-ply",{attrs:{src:t.publicPath+"depth2pcd_GT_"+t.objType+".ply",height:320,width:320}})],1)]),a("div",{staticClass:"float-container",staticStyle:{"text-align":"center"}},[a("h2",{attrs:{id:"Acknowledgements"}},[t._v(" Acknowledgements ")]),a("p",{staticStyle:{"text-align":"left","margin-left":"40px","margin-right":"40px"}},[t._v("Animesh Garg is a CIFAR AI Chair. Alan Aspuru-Guzik is a CIFAR AI Chair and CIFAR Lebovic Fellow. Animesh Garg and Florian Shkurti are also supported in part through the NSERC Discovery Grants Program. The authors would like to acknowledge Vector Institute and ComputeCanada for computing services. Alan Aspuru-Guzik and Haoping Xu thank the Canada 150 Research Chair funding from NSERC, Canada. Alan Aspuru-Guzik is thankful for the generous support of Dr. Anders G. Froseth. The authors would like to thank Yuchi Zhao for constructive feedback and discussions on the manuscript.")]),a("center",[a("div",{staticClass:"float-mini"},[a("v-img",{attrs:{src:t.publicPath+"toronto.jpg",contain:""}})],1),a("div",{staticClass:"float-mini"},[a("v-img",{attrs:{src:t.publicPath+"vector.jpg",contain:""}})],1),a("div",{staticClass:"float-minih"},[a("v-img",{attrs:{src:t.publicPath+"pair.png",contain:""}})],1),a("div",{staticClass:"float-minih"},[a("v-img",{attrs:{src:t.publicPath+"matter.svg",contain:""}})],1)])],1)],1)},o=[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"topnav",staticStyle:{"text-align":"right"},attrs:{id:"myTopnav"}},[a("a",{staticClass:"active",attrs:{href:"#home"}},[t._v("5th Conference on Robot Learning (CoRL 2021)")]),a("a",{attrs:{href:"#Architecture"}},[t._v("Architecture")]),a("a",{attrs:{href:"#Collection"}},[t._v("Dataset Collection")]),a("a",{attrs:{href:"#Dataset"}},[t._v("Dataset")]),a("a",{attrs:{href:"#Explorer"}},[t._v("Dataset Explorer")]),a("a",{attrs:{href:"#Acknowledgements"}},[t._v("Acknowledgements")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("p",{staticStyle:{color:"#565656"}},[a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:"https://www.linkedin.com/in/haoping-xu-0b3b44a4/"}},[t._v("Haoping Xu")]),a("sup",[t._v("* 1,2")]),t._v(", "),a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:"https://www.linkedin.com/in/yi-ru-helen-wang/"}},[t._v("Yi Ru Wang")]),a("sup",[t._v("* 1,2")]),t._v(", "),a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:""}},[t._v("Sagi Eppel")]),a("sup",[t._v("1,2")]),t._v(", "),a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:"https://www.matter.toronto.edu/basic-content-page/about-alan"}},[t._v("Alan Aspuru-Guzik")]),a("sup",[t._v("1,2")]),t._v(", "),a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:"http://www.cs.toronto.edu/~florian/"}},[t._v("Florian Shkurti")]),a("sup",[t._v("1,2")]),t._v(", "),a("a",{staticStyle:{color:"#565656","font-size":"1.5vw"},attrs:{href:"https://animesh.garg.tech/"}},[t._v("Animesh Garg")]),a("sup",[t._v("1,2,3")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"col",staticStyle:{"font-size":"1vw"}},[a("p",[a("sup",[t._v("1")]),t._v("University of Toronto   "),a("sup",[t._v("2")]),t._v("Vector Institute   "),a("sup",[t._v("3")]),t._v("NVIDIA   "),a("sup",[t._v("*")]),t._v("Equal Contribution   ")])])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticStyle:{"margin-top":"6rem","margin-bottom":"6rem"}},[a("iframe",{attrs:{width:"640",height:"480",src:"https://www.youtube.com/embed/ELzNzE48FFY",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:""}})])},function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"float-child-half"},[a("h2",{attrs:{id:"Collection"}},[t._v(" Automated Dataset collection ")]),a("iframe",{attrs:{width:"500",height:"300",src:"https://www.youtube.com/embed/_HIetJ4mdlg",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:""}}),a("p",{attrs:{STYLE:"text-align:left;font-size:10.0pt;margin-left:20px;margin-right:80px;"}},[t._v(" A commodity RGB-D sensor is mounted to the robot arm's end effector. The scene with the transparent object is scanned from multiple viewing angles to collect the raw depth. AprilTags on the base template are detected for each image, and based on their 6DoF poses and the known translation between tags and objects, we can fit the 3D model of object(s) to their respective locations. The result of this automatic collection and annotation process is the RGB image, instance object segmentation, raw depth, ground truth depth.")])])}],s=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("img",{attrs:{src:""+t.publicPath+t.file}})},n=[],l={name:"GifViewer",props:{file:String},data:function(){return{publicPath:""}}},c=l,h=(a("c25a"),a("2877")),p=Object(h["a"])(c,s,n,!1,null,"1eaf9769",null),d=p.exports,u=a("e360"),f={name:"App",components:{GifViewer:d,ModelStl:u["ModelStl"],ModelPly:u["ModelPly"]},data:function(){return{objType:0,items:[{name:"Beaker 0",index:0},{name:"Beaker 1",index:1},{name:"Beaker 2",index:2},{name:"Flask 0",index:3},{name:"Flask 1",index:4},{name:"Falsk 2",index:5}],publicPath:"",scale:{x:200,y:0,z:-3},videoId:"https://www.youtube.com/watch?v=mfL8tZUKRW4",playerVars:{autoplay:1}}}},v=f,m=(a("034f"),a("6544")),g=a.n(m),b=a("e4e5"),w=a("8336"),y=a("adda"),_=Object(h["a"])(v,r,o,!1,null,null,null),T=_.exports;g()(_,{VBanner:b["a"],VBtn:w["a"],VImg:y["a"]});var j=a("f309");i["default"].use(j["a"]);var x=new j["a"]({});i["default"].config.productionTip=!1,new i["default"]({vuetify:x,render:function(t){return t(T)}}).$mount("#app")},5962:function(t,e,a){},"85ec":function(t,e,a){},c25a:function(t,e,a){"use strict";a("5962")}});
//# sourceMappingURL=app.a6367120.js.map